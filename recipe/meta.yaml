{% set cudnn_version= "9.1.1.17" %}
{% set build_number= "0" %}
{% set platform = "linux-x86_64" %}  # [linux64]
{% set platform = "linux-ppc64le" %}  # [ppc64le]
{% set platform = "linux-sbsa" %}  # [aarch64]
{% set platform = "windows-x86_64" %}  # [win]
{% set extension = "tar.xz" %}  # [not win]
{% set extension = "zip" %}  # [win]


package:
  name: cudnn
  version: {{ cudnn_version }}

source:
  url: https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/{{ platform }}/cudnn-{{ platform }}-{{ cudnn_version }}_cuda{{ cuda_maj_version }}-archive.{{ extension }}
  sha256: 15a8b77123c1911606b45703691ad0990892c6098070ffab0bcaa003183a7dcb  # [linux64 and (cuda_maj_version == 11)]
  sha256: 992b4be26899cc4c618bb1f6989261df7d0a9f9032b2217bf1fce9dd3228c904  # [linux64 and (cuda_maj_version == 12)]
  sha256: 534984ea42f9e754d78aace88d6b5bed124c2557d84350c9488776e4d0dffa56  # [aarch64 and (cuda_maj_version == 11)]
  sha256: 19bd66ee9fb30348f18801a398d0bec98b4663866efa244ca122825b3429526c  # [aarch64 and (cuda_maj_version == 12)]
  sha256: 58cec56a754ffdea3d19c5bec88c4cbbf4d00b94fe1ecbf10a3b3b8fa1d1b00e  # [win and (cuda_maj_version == 11)]
  sha256: 3f1efb1a82e480594594e8b1fc8075a4852b4ada5a1fea3c8ec4b3ed1e5122ed  # [win and (cuda_maj_version == 12)]

build:
  skip: True  # [osx or (linux and s390x)]
  number: {{ build_number }}
  string: cuda{{ cuda_maj_version }}_{{ build_number }}
  missing_dso_whitelist:
    - '*'
  run_exports:
    - {{ pin_subpackage('cudnn', min_pin='x.x', max_pin='x') }}

requirements:
  run:
    - cudatoolkit {{ cuda_maj_version }}.*   # [cuda_maj_version == 11]
    - cuda-version {{ cuda_maj_version }}.*
    - cuda-nvrtc                             # [cuda_maj_version == 12]
    - libcublas                              # [cuda_maj_version == 12]
    # libcudnn_cnn_infer.so needs libm.so.6 with ABI v2.27, shown when test_load_elf is run. So we include this __glibc
    # restriction. This means, though, that we can't install cudnn in our linux-aarch64 builder containers (which have
    # glibc v2.26), including to run the tests while building the package. However, we don't currently build anything for
    # linux-aarch64 CUDA - and when we start, a solver error will flag up this issue to be handled appropriately at that
    # time.
    - __glibc >=2.27                         # [linux and aarch64]

test:
  requires:
    - {{ compiler('c') }}                                                                                        # [linux and x86_64]
  files:
    - test_load_elf.c                                                                                            # [linux and x86_64]
  commands:
    - if not exist %LIBRARY_INC%\\cudnn.h exit 1                                                                 # [win]
    - if not exist %LIBRARY_INC%\\cudnn_adv.h exit 1                                                             # [win]
    - if not exist %LIBRARY_BIN%\\cudnn64_{{ '.'.join(cudnn_version.split('.')[0:1]) }}.dll exit 1               # [win]
    - if not exist %LIBRARY_BIN%\\cudnn_adv64_{{ '.'.join(cudnn_version.split('.')[0:1]) }}.dll exit 1           # [win]
    - test -f ${PREFIX}/include/cudnn.h                                                                          # [linux and x86_64]
    - test -f ${PREFIX}/include/cudnn_adv.h                                                                      # [linux and x86_64]
    - test -f ${PREFIX}/lib/libcudnn.so                                                                          # [linux and x86_64]
    - test -f ${PREFIX}/lib/libcudnn_adv.so                                                                      # [linux and x86_64]
    - test -f ${PREFIX}/lib/libcudnn.so.{{ '.'.join(cudnn_version.split('.')[0:1]) }}                            # [linux and x86_64]
    - test -f ${PREFIX}/lib/libcudnn.so.{{ '.'.join(cudnn_version.split('.')[0:3]) }}                            # [linux and x86_64]
    - ${GCC} test_load_elf.c -std=c99 -Werror -ldl -o test_load_elf                                              # [linux and x86_64]
    - for f in ${PREFIX}/lib/libcudnn*.so; do ./test_load_elf $f; done                                           # [linux and x86_64]

about:
  home: https://developer.nvidia.com/cudnn
  license_file: LICENSE
  license_family: PROPRIETARY
  license: Proprietary
  summary: "NVIDIA's cuDNN deep neural network acceleration library"
  description: |
    The NVIDIA CUDAÂ® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural
    networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward
    convolution, pooling, normalization, and activation layers.
  doc_url: https://docs.nvidia.com/deeplearning/cudnn
  dev_url: https://developer.nvidia.com/rdp/cudnn-download
